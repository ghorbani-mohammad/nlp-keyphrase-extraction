{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     2,
     7,
     9,
     12,
     25,
     35,
     47,
     49,
     65
    ]
   },
   "outputs": [],
   "source": [
    "class TextRank4Keyword:\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    def __init__(self):\n",
    "        self.d = 0.85  # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5  # convergence threshold  0.00001\n",
    "        self.steps = 10  # iteration steps\n",
    "        self.node_weight = None  # save keywords and its weight\n",
    "    def set_stopwords(self, stopwords):\n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i + 1, i + window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "\n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "\n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm != 0)  # this is ignore the 0 element in norm\n",
    "        return g_norm\n",
    "    def get_keywords(self, file_name, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        outFile = open('Input/result_'+file_name, 'w', encoding=\"utf8\")\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            outFile.write(key + ' - ' + str(value))\n",
    "            outFile.write(\"\\n\")\n",
    "            if i > number:\n",
    "                break\n",
    "        outFile.close()                \n",
    "    def analyze(self, file_name, text, candidate_pos=['NOUN', 'PROPN', 'VERB'], window_size=3, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "\n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "\n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower)  # list of list of words\n",
    "\n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "\n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "\n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "\n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "\n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1 - self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr)) < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "\n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def analyze_files():\n",
    "    dir_path = '.\\\\Input'\n",
    "    all_files = os.listdir(dir_path)\n",
    "    txt_files = list(filter(lambda x: x[-4:] == '.txt', all_files))\n",
    "    for file in txt_files:\n",
    "        text = open(dir_path+'\\\\'+file, \"r\", errors='ignore')\n",
    "        tr4w = TextRank4Keyword()\n",
    "        tr4w.analyze(file, text.read(), candidate_pos = ['NOUN', 'PROPN', 'VERB'], window_size=4, lower=False)\n",
    "        print(\"------> Keywords of \"+file+\" are:\")\n",
    "        tr4w.get_keywords(file, 10)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------> Keywords of i1.txt are:\n",
      "Roundup - 3.474717819589368\n",
      "California - 3.346399196919054\n",
      "Monsanto - 2.781928006449759\n",
      "Bayer - 2.735789458605436\n",
      "award - 2.5375336733643254\n",
      "regulators - 2.33947138553843\n",
      "Smith - 2.333728633670195\n",
      "based - 2.285202703229221\n",
      "damages - 2.035912061427344\n",
      "jury - 1.9095770674633439\n",
      "said - 1.8933706144878424\n",
      "case - 1.8264373794404167\n",
      "\n",
      "------> Keywords of i2.txt are:\n",
      "Twitter - 5.247483003298711\n",
      "users - 4.905279028016895\n",
      "% - 3.2785649339703475\n",
      "revenue - 3.1249577594239137\n",
      "platform - 2.9848391408003145\n",
      "said - 2.887012402269453\n",
      "quarter - 2.4720092789584727\n",
      "site - 2.1065516876253\n",
      "company - 2.0967995060266227\n",
      "content - 2.0688847019873498\n",
      "growth - 1.7616043639941767\n",
      "year - 1.7234142272246213\n",
      "\n",
      "------> Keywords of i3.txt are:\n",
      "% - 6.715146696397326\n",
      "growth - 5.349439414528776\n",
      "economy - 4.133499903732388\n",
      "Trump - 3.5557031423111276\n",
      "tax - 3.1403892904153192\n",
      "year - 3.074499665148791\n",
      "quarter - 3.0535080509077646\n",
      "measure - 2.163811606661924\n",
      "cuts - 2.1373967221490977\n",
      "rate - 2.011096375964068\n",
      "GDP - 1.9151465844726885\n",
      "months - 1.7612898737302296\n",
      "\n",
      "------> Keywords of i4.txt are:\n",
      "said - 7.203550356032391\n",
      "immigration - 4.8228665458442155\n",
      "ICE - 4.242613926854089\n",
      "cases - 4.233502953680738\n",
      "family - 3.725193990934622\n",
      "people - 3.715049513475441\n",
      "court - 3.4801008089810685\n",
      "notice - 3.4166216620743133\n",
      "deportation - 3.325563262617885\n",
      "families - 2.6982831768291766\n",
      "docket - 2.697477435182034\n",
      "U.S. - 2.611931994592778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyze_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
